# Ollama Configuration
# The model you are running in Ollama (e.g., "llama3", "codellama", "mistral", "glm-4.6:cloud", etc.)
OLLAMA_MODEL=glm-4.6:cloud

# The address where your Ollama server is running. Default is usually correct.
OLLAMA_HOST=http://localhost:11434
